{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"\"\"I’m amazed how often in practice, not only does a @huggingface NLP model solve your problem, but one of their public finetuned checkpoints, is good enough for the job.\n",
    "\n",
    "Both impressed, and a little disappointed how rarely I get to actually train a model that matters :(\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pedro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i’m',\n",
       " 'amazed',\n",
       " 'how',\n",
       " 'often',\n",
       " 'in',\n",
       " 'practice,',\n",
       " 'not',\n",
       " 'only',\n",
       " 'does',\n",
       " 'a',\n",
       " '@huggingface',\n",
       " 'nlp',\n",
       " 'model',\n",
       " 'solve',\n",
       " 'your',\n",
       " 'problem,',\n",
       " 'but',\n",
       " 'one',\n",
       " 'of',\n",
       " 'their',\n",
       " 'public',\n",
       " 'finetuned',\n",
       " 'checkpoints,',\n",
       " 'is',\n",
       " 'good',\n",
       " 'enough',\n",
       " 'for',\n",
       " 'the',\n",
       " 'job.',\n",
       " 'both',\n",
       " 'impressed,',\n",
       " 'and',\n",
       " 'a',\n",
       " 'little',\n",
       " 'disappointed',\n",
       " 'how',\n",
       " 'rarely',\n",
       " 'i',\n",
       " 'get',\n",
       " 'to',\n",
       " 'actually',\n",
       " 'train',\n",
       " 'a',\n",
       " 'model',\n",
       " 'that',\n",
       " 'matters',\n",
       " ':(']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = tweet.lower().split()\n",
    "\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With stopwords: i’m amazed how often in practice, not only does a @huggingface nlp model solve your problem, but one of their public finetuned checkpoints, is good enough for the job. both impressed, and a little disappointed how rarely i get to actually train a model that matters :(\n",
      "Without: i’m amazed often practice, @huggingface nlp model solve problem, one public finetuned checkpoints, good enough job. impressed, little disappointed rarely get actually train model matters :(\n"
     ]
    }
   ],
   "source": [
    "tweet_no_stopwords = [word for word in tweet if word not in stop_words]\n",
    "\n",
    "print(\"With stopwords:\", ' '.join(tweet))\n",
    "print(\"Without:\", ' '.join(tweet_no_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically in NLP we will find that models consume a token, which can represent a multitude of different things, such as:\n",
    "\n",
    "A word\n",
    "Part of a word\n",
    "A single character\n",
    "Puntuation mark [,!-.]\n",
    "Special token like <URL>, or <NAME>\n",
    "Model-specific special tokens, like [CLS] and [SEP] for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the BERT transformer model there are *five* special tokens that are used by the model, these are:\n",
    "\n",
    "| Token | Meaning |\n",
    "| --- | --- |\n",
    "| **[PAD]** | Padding token, allows us to maintain same-length sequences (512 tokens for Bert) even when different sized sentences are fed in |\n",
    "| **[UNK]** | Used when a word is unknown to Bert |\n",
    "| **[CLS]** | Appears at the start of every sequence |\n",
    "| **[SEP]** | Indicates a seperator or end of sequence |\n",
    "| **[MASK]** | Used when masking tokens, for example in training with masked language modelling (MLM) |\n",
    "\n",
    "So if we take the *'NLP models'* tweet, processing that directly with our BERT specific tokens might look like this:\n",
    "\n",
    "```\n",
    "['[CLS]', '[UNK]', 'thinks', 'that', 'the', 'nlp', 'models', 'that', '[UNK]', 'made', 'are', 'super', 'cool', '[SEP]', '[PAD]', '[PAD]', ..., '[PAD]']\n",
    "```\n",
    "\n",
    "Here, we have:\n",
    "\n",
    "* Applied **\\[CLS\\]** token to indicate the start of the sequence.\n",
    "* Both username tokens *@elonmusk* and *@joebloggs* were not 'known' words to BERT so BERT replaced them with unknown tokens **\\[UNK\\]**, alternatively we could have replaced these with our own special **user** tokens.\n",
    "* Added **\\[SEP\\]* token to the end of our sequence.\n",
    "* Padded the sequence upto the required length of 512 tokens *(required due to fixed input sequence length of BERT model)* using **\\[PAD\\]** tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"I am amazed by how amazingly amazing you are\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use different forms of the word amaze a total of three times. Each of these different forms is called an 'inflection', which is the modification of a word to slightly adjust the meaning or context of the word. When we tokenize this text we produce three different tokens for each inflection of happy, which is okay but in many applications this level of granularity in the semantic meaning of the word is not required and can damage model performance.\n",
    "\n",
    "Later, when we get to using more complex, sophisticated models (eg BERT), we will use different methods that maintain the inflection of each word - but it is important to understand stemming as it was a very important part of text preprocessing for a very long time, and still relevant to many applications.\n",
    "\n",
    "To apply stemming we will be using the NLTK package, which provides several different stemmers, we will test the PorterStemmer and LancasterStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter | Lancaster\n",
      "happi | happy\n",
      "happiest | happiest\n",
      "happier | happy\n",
      "cactu | cact\n",
      "cactii | cacti\n",
      "eleph | eleph\n",
      "eleph | eleph\n",
      "amaz | amaz\n",
      "amaz | amaz\n",
      "amazingli | amaz\n",
      "cement | cem\n",
      "owe | ow\n",
      "maximum | maxim\n"
     ]
    }
   ],
   "source": [
    "words_to_stem = ['happy', 'happiest', 'happier', 'cactus', 'cactii', 'elephant', 'elephants', 'amazed', 'amazing', 'amazingly', 'cement', 'owed', 'maximum']\n",
    "\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "stemmed = [(porter.stem(word), lancaster.stem(word)) for word in words_to_stem]\n",
    "\n",
    "print(\"Porter | Lancaster\")\n",
    "for stem in stemmed:\n",
    "    print(f\"{stem[0]} | {stem[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is very similiar to stemming in that it reduces a set of inflected words down to a common word. The difference is that lemmatization reduces inflections down to their real root words, which is called a lemma. If we take the words 'amaze', 'amazing', 'amazingly', the lemma of all of these is 'amaze'. Compared to stemming which would usually return 'amaz'. Generally lemmatization is seen as more advanced than stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['amaze', 'amazed', 'amazing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pedro\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amaze', 'amaze', 'amaze']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "[lemmatizer.lemmatize(word, wordnet.VERB) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "Unicode normalization is used to *normalize* different but similiar characters. For example the following unicode characters (and character combinations) are equivalent:\n",
    "\n",
    "**Canonical Equivalence**\n",
    "\n",
    "| | | Equivalence Reason |\n",
    "| --- | --- | --- |\n",
    "| Ç | C◌̧ | Combined character sequences |\n",
    "| 가 | ᄀ ᅡ | Conjoined Korean characters |\n",
    "\n",
    "**Compatibility equivalence**\n",
    "\n",
    "| | | Equivalence Reason |\n",
    "| --- | --- | --- |\n",
    "| ℌ | H | Font variant |\n",
    "| \\[NBSP\\] | \\[SPACE\\] | Both are linebreak sequences |\n",
    "| ① | 1 | Circled variant |\n",
    "| x² | x2 | Superscript |\n",
    "| xⱼ | xj | Subscript |\n",
    "| ½ | 1/2 | Fractions |\n",
    "\n",
    "We have mentioned two different types of equivalence here, canonical and compatibility equivalence.\n",
    "\n",
    "**Canonical equivalence** means both forms are fundamentally the same and when rendered are indistinguishable. For example we can take the unicode for `'Ç' \\u00C7` or the unicode for `'C' \\u0043` and `'̧' \\u0327`, when the latter two characters are rendered together they look the same as the first character:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is in these cases that we use unicode normalization to *normalize* our characters into matching pairs. As there are different forms of equivalence, there are also different forms of normalization. These are all called **N**ormal **F**orm, and there are four different methods:\n",
    "\n",
    "| Name | Abbreviation | Description | Example |\n",
    "| --- | --- | --- | --- |\n",
    "| Form D | NFD | *Canonical* decomposition | `Ç` → `C ̧` |\n",
    "| Form C | NFC | *Canoncial* decomposition followed by *canonical* composition | `Ç` → `C ̧` → `Ç` |\n",
    "| Form KD | NFKD | *Compatibility* decomposition | `ℌ ̧` → `H ̧` |\n",
    "| Form KC | NFKC | *Compatibility* decomposition followed by *canonical* composition | `ℌ ̧` → `H ̧` → `Ḩ` |\n",
    "\n",
    "Let's take a look at each of these forms in action. Our C with cedilla character Ç can be represented in two ways, as a single character called *Latin capital C with cedilla* (*\\u00C7*), or as two characters called *Latin capital C* (*\\u0043*) and *combining cedilla* (*\\u0327*):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NFD and NFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ç'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "c_with_cedilla = \"\\u00C7\"  # Latin capital C with cedilla (single character)\n",
    "c_with_cedilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ç'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_plus_cedilla = \"\\u0043\\u0327\"  # \\u0043 = Latin capital C, \\u0327 = 'combining cedilla' (two characters)\n",
    "c_plus_cedilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we perform NFD on our C with cedilla character \\u00C7, we decompose the character into it's smaller components, which are the Latin capital C character, and combining cedilla character \\u0043 + \\u0327. This means that if we compare an NFD normalized C with cedilla character to both the C character and the cedilla character, we will return true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFD', c_with_cedilla) == c_plus_cedilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we perform NFC on our C with cedilla character \\u00C7, we decompose the character into the smaller components \\u0043 + \\u0327, and then compose them back to \\u00C7, and so they will not match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFC', c_with_cedilla) == c_plus_cedilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we switch the NFC encoding to instead be performed on our two characters \\u0043 + \\u0327, they will first be decomposed (which will do nothing as they are already decomposed), then compose them into the single \\u00C7 character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_with_cedilla == unicodedata.normalize('NFC', c_plus_cedilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NFKD and NFKC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NFK encodings do not decompose characters into smaller components, they decompose characters into their normal versions. For example if we take the fancy format ℌ \\u210B, we cannot decompose this into multiple characters and so NFD or NFC encoding will do nothing. However, if we apply NFKD, we will find that our fancy ℌ \\u210B becomes a plain, boring H \\u0048:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFKD', 'ℌ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ℋ̧'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\u210B\\u0327\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying our compatibility decomposition normalization (NFKD) gives us a capital H character, and a combining cedilla character as two seperate encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'H\\xcc\\xa7'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFKD', \"\\u210B\\u0327\").encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we apply NFKC, we first perform compatibility decomposition, into the two seperate characters, before merging them during canonical composition:unicodedata.normalize('NFKC', \"\\u210B\\u0327\").encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xe1\\xb8\\xa8'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFKC', \"\\u210B\\u0327\").encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the only difference between these two methods is a canonical composition, we see no difference between the two character sets when they are rendered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ḩ', 'Ḩ')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFKC', \"\\u210B\\u0327\"), unicodedata.normalize('NFKD', \"\\u210B\\u0327\"), "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
